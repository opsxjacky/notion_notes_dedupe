#!/usr/bin/env python3
"""
Notion è‹¹æœç¬”è®°æ•°æ®åº“å»é‡è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
1. è®¾ç½®ç¯å¢ƒå˜é‡ NOTION_TOKEN (Notion Integration Token)
2. ä¿®æ”¹ DATABASE_ID ä¸ºä½ çš„è‹¹æœç¬”è®°åŒæ­¥æ•°æ®åº“ ID
3. è¿è¡Œ: python notion_dedupe.py

å»é‡é€»è¾‘:
- æŒ‰ã€Œåç§°ã€å­—æ®µè¯†åˆ«é‡å¤
- ä¿ç•™æœ€æ–°çš„è®°å½•ï¼ˆæŒ‰åˆ›å»ºæ—¶é—´ï¼‰
- å½’æ¡£/åˆ é™¤è¾ƒæ—§çš„é‡å¤è®°å½•
"""

import os
import sys
import argparse
import requests
from collections import defaultdict
from datetime import datetime

# é…ç½®
NOTION_TOKEN = os.environ.get("NOTION_TOKEN")
if not NOTION_TOKEN:
    print("é”™è¯¯: è¯·è®¾ç½®ç¯å¢ƒå˜é‡ NOTION_TOKEN")
    print("ä¾‹å¦‚: export NOTION_TOKEN='your_token_here'")
    exit(1)

DATABASE_ID = "2df4538c-fc22-80a8-a9c2-e213711c1efa"  # è‹¹æœç¬”è®°åŒæ­¥æ•°æ®åº“

HEADERS = {
    "Authorization": f"Bearer {NOTION_TOKEN}",
    "Content-Type": "application/json",
    "Notion-Version": "2022-06-28"
}

def query_database(database_id, start_cursor=None):
    """æŸ¥è¯¢æ•°æ®åº“æ‰€æœ‰è®°å½•"""
    url = f"https://api.notion.com/v1/databases/{database_id}/query"
    payload = {"page_size": 100}
    if start_cursor:
        payload["start_cursor"] = start_cursor
    
    response = requests.post(url, headers=HEADERS, json=payload)
    response.raise_for_status()
    return response.json()

def get_all_pages(database_id):
    """è·å–æ•°æ®åº“ä¸­çš„æ‰€æœ‰é¡µé¢"""
    all_pages = []
    has_more = True
    start_cursor = None
    
    while has_more:
        result = query_database(database_id, start_cursor)
        all_pages.extend(result.get("results", []))
        has_more = result.get("has_more", False)
        start_cursor = result.get("next_cursor")
    
    return all_pages

def extract_page_info(page):
    """ä»é¡µé¢ä¸­æå–å…³é”®ä¿¡æ¯"""
    page_id = page["id"]
    
    # è·å–æ ‡é¢˜
    title = ""
    title_prop = page.get("properties", {}).get("åç§°", {})
    if title_prop.get("title"):
        title = "".join([t.get("plain_text", "") for t in title_prop["title"]])
    
    # è·å–æ­£æ–‡
    content = ""
    content_prop = page.get("properties", {}).get("æ­£æ–‡", {})
    if content_prop.get("rich_text"):
        content = "".join([t.get("plain_text", "") for t in content_prop["rich_text"]])
    
    # è·å–åˆ›å»ºæ—¶é—´
    created_time = page.get("created_time", "")
    
    return {
        "id": page_id,
        "title": title.strip(),
        "content": content.strip(),
        "created_time": created_time,
        "url": page.get("url", "")
    }

def find_duplicates(pages):
    """æ‰¾å‡ºé‡å¤çš„é¡µé¢"""
    # æŒ‰æ ‡é¢˜åˆ†ç»„
    by_title = defaultdict(list)
    for page in pages:
        if page["title"]:  # å¿½ç•¥ç©ºæ ‡é¢˜
            by_title[page["title"]].append(page)
    
    # æ‰¾å‡ºæœ‰é‡å¤çš„
    duplicates = {}
    for title, page_list in by_title.items():
        if len(page_list) > 1:
            # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
            sorted_pages = sorted(
                page_list, 
                key=lambda x: x["created_time"], 
                reverse=True
            )
            duplicates[title] = {
                "keep": sorted_pages[0],      # ä¿ç•™æœ€æ–°çš„
                "remove": sorted_pages[1:]     # åˆ é™¤å…¶ä»–çš„
            }
    
    return duplicates

def archive_page(page_id):
    """å½’æ¡£ï¼ˆè½¯åˆ é™¤ï¼‰é¡µé¢"""
    url = f"https://api.notion.com/v1/pages/{page_id}"
    payload = {"archived": True}
    
    response = requests.patch(url, headers=HEADERS, json=payload)
    response.raise_for_status()
    return response.json()

def main():
    parser = argparse.ArgumentParser(description="Notion è‹¹æœç¬”è®°æ•°æ®åº“å»é‡è„šæœ¬")
    parser.add_argument("--dry-run", action="store_true", help="ä»…é¢„è§ˆï¼Œä¸å®é™…æ‰§è¡Œå½’æ¡£")
    parser.add_argument("--auto", action="store_true", help="è‡ªåŠ¨æ‰§è¡Œï¼Œæ— éœ€ç¡®è®¤ï¼ˆç”¨äº CI/CDï¼‰")
    args = parser.parse_args()
    
    print("ğŸ” æ­£åœ¨æŸ¥è¯¢è‹¹æœç¬”è®°åŒæ­¥æ•°æ®åº“...")
    
    # è·å–æ‰€æœ‰é¡µé¢
    all_pages = get_all_pages(DATABASE_ID)
    print(f"ğŸ“ å…±æ‰¾åˆ° {len(all_pages)} æ¡è®°å½•")
    
    # æå–é¡µé¢ä¿¡æ¯
    pages_info = [extract_page_info(p) for p in all_pages]
    
    # æ‰¾å‡ºé‡å¤
    duplicates = find_duplicates(pages_info)
    
    if not duplicates:
        print("âœ… æ²¡æœ‰å‘ç°é‡å¤è®°å½•!")
        return
    
    print(f"\nâš ï¸  å‘ç° {len(duplicates)} ç»„é‡å¤è®°å½•:\n")
    
    total_to_remove = 0
    for title, dup_info in duplicates.items():
        keep = dup_info["keep"]
        remove_list = dup_info["remove"]
        total_to_remove += len(remove_list)
        
        print(f"ğŸ“‹ ã€Œ{title}ã€")
        keep_preview = keep['content'][:30] + "..." if keep['content'] else "(ç©º)"
        print(f"   âœ“ ä¿ç•™: {keep['created_time'][:10]} - {keep_preview}")
        for r in remove_list:
            r_preview = r['content'][:30] + "..." if r['content'] else "(ç©º)"
            print(f"   âœ— åˆ é™¤: {r['created_time'][:10]} - {r_preview}")
        print()
    
    # dry-run æ¨¡å¼
    if args.dry_run:
        print(f"ğŸ” [DRY-RUN] é¢„è§ˆæ¨¡å¼ï¼Œå…± {total_to_remove} æ¡è®°å½•å°†è¢«å½’æ¡£ï¼Œä½†ä¸ä¼šå®é™…æ‰§è¡Œ")
        return
    
    # è‡ªåŠ¨æ¨¡å¼ï¼ˆCI/CDï¼‰æˆ–äº¤äº’ç¡®è®¤
    if not args.auto:
        print(f"âš ï¸  å°†å½’æ¡£ {total_to_remove} æ¡é‡å¤è®°å½•")
        confirm = input("ç¡®è®¤æ‰§è¡Œ? (y/N): ").strip().lower()
        if confirm != 'y':
            print("âŒ å·²å–æ¶ˆ")
            return
    
    # æ‰§è¡Œå½’æ¡£
    print("\nğŸ—‘ï¸  æ­£åœ¨å½’æ¡£é‡å¤è®°å½•...")
    for title, dup_info in duplicates.items():
        for page in dup_info["remove"]:
            try:
                archive_page(page["id"])
                print(f"   âœ“ å·²å½’æ¡£: {title}")
            except Exception as e:
                print(f"   âœ— å½’æ¡£å¤±è´¥: {title} - {e}")
    
    print("\nâœ… å»é‡å®Œæˆ!")

if __name__ == "__main__":
    main()
